{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the path of each sub-package.\n",
    "from neuralplex_study.materials import MATERIALS_PATH\n",
    "from neuralplex_study.methods import METHODS_PATH\n",
    "from neuralplex_study.results import RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, randint\n",
    "from typing import List, Dict, Self\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, m: float, b: float = 0, step: float = None, name=None):\n",
    "        self.m = m  # weight\n",
    "        self.b = b  # lift\n",
    "        self.name = name\n",
    "        self.step = step\n",
    "        self.value = 0\n",
    "        self.activation_count = 0\n",
    "        self.neuronsRHS: List[Self] = []\n",
    "        self.neuronsLHS: List[Self] = []\n",
    "        self.activation: Dict[Self, float] = {}\n",
    "        self.propagation: Dict[Self, float] = {}\n",
    "\n",
    "    def connectRHS(self, neuronRHS: Self) -> None:\n",
    "        if neuronRHS not in self.neuronsRHS:\n",
    "            self.neuronsRHS.append(neuronRHS)\n",
    "        if self not in neuronRHS.neuronsLHS:\n",
    "            neuronRHS.connectLHS(self)\n",
    "\n",
    "    def connectLHS(self, neuronLHS: Self) -> None:\n",
    "        if neuronLHS not in self.neuronsLHS:\n",
    "            self.neuronsLHS.append(neuronLHS)\n",
    "        if self not in neuronLHS.neuronsRHS:\n",
    "            neuronLHS.connectRHS(self)\n",
    "\n",
    "    def disconnectRHS(self, neuronRHS: Self) -> None:\n",
    "        if neuronRHS in self.neuronsRHS:\n",
    "            self.neuronsRHS.remove(neuronRHS)\n",
    "        if self in neuronRHS.neuronsLHS:\n",
    "            neuronRHS.disconnectLHS(self)\n",
    "        if len(self.neuronsRHS) == 0:\n",
    "            for neuron in self.neuronsLHS:\n",
    "                neuron.disconnectRHS(self)\n",
    "\n",
    "    def disconnectLHS(self, neuronLHS: Self) -> None:\n",
    "        if neuronLHS in self.neuronsLHS:\n",
    "            self.neuronsLHS.remove(neuronLHS)\n",
    "        if self in neuronLHS.neuronsRHS:\n",
    "            neuronLHS.disconnectRHS(self)\n",
    "        if len(self.neuronsLHS) == 0:\n",
    "            for neuron in self.neuronsRHS:\n",
    "                neuron.disconnectLHS(self)\n",
    "\n",
    "    def activate(self, value: float, neuron: Self = None) -> None:\n",
    "        if self.activation_count == 0:\n",
    "            self.activation = {}\n",
    "        self.activation_count = self.activation_count + 1\n",
    "        self.activation.update({neuron: value})\n",
    "        if len(self.neuronsLHS) == 0 or self.activation_count == len(self.neuronsLHS):\n",
    "            self.value = self.m * sum(self.activation.values())\n",
    "            for neuron in self.neuronsRHS:\n",
    "                neuron.activate(self.value, self)\n",
    "                # Under what conditions should a numeric activation value be propagated?\n",
    "            self.activation_count = 0\n",
    "\n",
    "    def propagate(self, error: float, neuron: Self = None) -> None:\n",
    "        self.propagation.update({neuron: error})\n",
    "        if len(self.neuronsRHS) == 0 or len(self.propagation) == len(self.neuronsRHS):\n",
    "            for error in self.propagation.values():\n",
    "                self.m = self.m - (error * self.step)\n",
    "                # This is a primitive implementation; however, I have some ideas for how to improve it.\n",
    "                # Please let me know if you have any thoughts on how this should be implemented.\n",
    "            error_total = sum(self.propagation.values())\n",
    "            for neuron in self.neuronsLHS:\n",
    "                neuron_activation_value = self.activation[neuron]\n",
    "                if (\n",
    "                    neuron_activation_value > 0\n",
    "                    and error_total > 0\n",
    "                    or neuron_activation_value < 0\n",
    "                    and error_total < 0\n",
    "                ):\n",
    "                    neuron.propagate(error_total, self)\n",
    "                else:\n",
    "                    neuron.propagate(math.copysign(1, error_total), self)\n",
    "                # Likewise, the \"backpropagation\" still needs a lot of work.\n",
    "            self.propagation = {}\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, neurons: List[Neuron], step: float = None):\n",
    "        self.neurons = neurons\n",
    "        self.step = step\n",
    "        if not self.step is None:\n",
    "            for neuron in self.neurons:\n",
    "                if neuron.step is None:\n",
    "                    neuron.step = self.step\n",
    "\n",
    "    def connect(self, layer: Self) -> None:\n",
    "        for p1 in self.neurons:\n",
    "            for p2 in layer.neurons:\n",
    "                p1.connectRHS(p2)\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "        self.input_layer = layers[0]\n",
    "        self.output_layer = layers[len(layers) - 1]\n",
    "        for i in range(0, len(layers) - 1):\n",
    "            l1 = layers[i]\n",
    "            l2 = layers[i + 1]\n",
    "            l1.connect(l2)\n",
    "\n",
    "    def train(self, X_train: List[float], y_train: List[float]) -> None:\n",
    "        if len(self.input_layer.neurons) != len(X_train):\n",
    "            raise Exception(\n",
    "                f\"The length of the input training values, {len(X_train)}, is not equal to the length of input neurons: {len(self.input_layer.neurons)}\"\n",
    "            )\n",
    "        if len(self.output_layer.neurons) != len(y_train):\n",
    "            raise Exception(\n",
    "                f\"The length of the output training values, {len(y_train)}, is not equal to the length of output neurons: {len(self.output_layer.neurons)}\"\n",
    "            )\n",
    "\n",
    "        # Activation Stage\n",
    "        for i in range(0, len(X_train)):\n",
    "            self.input_layer.neurons[i].activate(X_train[i], None)\n",
    "\n",
    "        # Backpropagation Stage\n",
    "        for i in range(0, len(y_train)):\n",
    "            y_train = y_train[i]\n",
    "            neuron = self.output_layer.neurons[i]\n",
    "            neuron.propagate(neuron.value - y_train, None)\n",
    "\n",
    "    def predict(self, X: List[float]) -> List[float]:\n",
    "        if len(self.input_layer.neurons) != len(X):\n",
    "            raise Exception(\n",
    "                f\"The length of the input values, {len(X)}, is not equal to the length of input neurons: {len(self.input_layer.neurons)}\"\n",
    "            )\n",
    "\n",
    "        for i in range(0, len(X)):\n",
    "            self.input_layer.neurons[i].activate(X[i])\n",
    "\n",
    "        return [neuron.value for neuron in self.output_layer.neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Neuron:\n",
    "\n",
    "#     def __init__(self, m: float, b: float = 0, step: float = None, name=None, activation_function=None):\n",
    "#         self.m = m  # weight\n",
    "#         self.b = b  # bias\n",
    "#         self.name = name\n",
    "#         self.step = step\n",
    "#         self.value = 0\n",
    "#         self.activation_count = 0\n",
    "#         self.neuronsRHS: List[Self] = []\n",
    "#         self.neuronsLHS: List[Self] = []\n",
    "#         self.activation: Dict[Self, float] = {}\n",
    "#         self.propagation: Dict[Self, float] = {}\n",
    "#         self.activation_function = activation_function or self.sigmoid  # Default to sigmoid\n",
    "\n",
    "#     def connectRHS(self, neuronRHS: Self) -> None:\n",
    "#         if neuronRHS not in self.neuronsRHS:\n",
    "#             self.neuronsRHS.append(neuronRHS)\n",
    "#         if self not in neuronRHS.neuronsLHS:\n",
    "#             neuronRHS.connectLHS(self)\n",
    "\n",
    "#     def connectLHS(self, neuronLHS: Self) -> None:\n",
    "#         if neuronLHS not in self.neuronsLHS:\n",
    "#             self.neuronsLHS.append(neuronLHS)\n",
    "#         if self not in neuronLHS.neuronsRHS:\n",
    "#             neuronLHS.connectRHS(self)\n",
    "\n",
    "#     def activate(self, value: float, neuron: Self = None) -> None:\n",
    "#         # Activation based on current value and weight\n",
    "#         self.activation_count += 1\n",
    "#         self.activation[neuron] = value\n",
    "\n",
    "#         if len(self.neuronsLHS) == 0 or self.activation_count == len(self.neuronsLHS):\n",
    "#             # Calculate the weighted sum\n",
    "#             self.value = self.m * sum(self.activation.values()) + self.b\n",
    "#             self.value = self.activation_function(self.value)  # Apply the activation function\n",
    "#             for neuron in self.neuronsRHS:\n",
    "#                 neuron.activate(self.value, self)\n",
    "#             self.activation_count = 0\n",
    "\n",
    "#     def propagate(self, error: float, neuron: Self = None) -> None:\n",
    "#         # Use the derivative of the activation function for backpropagation\n",
    "#         derivative = self.activation_derivative(self.value)\n",
    "\n",
    "#         # Adjust the weight based on the error and learning rate\n",
    "#         self.m -= self.step * error * derivative\n",
    "\n",
    "#         # Sum all error contributions from LHS neurons\n",
    "#         error_total = error * derivative\n",
    "#         for neuron in self.neuronsLHS:\n",
    "#             # Propagate the error backwards to the previous layer\n",
    "#             neuron.propagate(error_total, self)\n",
    "\n",
    "#         self.propagation.clear()\n",
    "\n",
    "#     def activation_derivative(self, value: float) -> float:\n",
    "#         \"\"\" Derivative of the activation function (Sigmoid by default) \"\"\"\n",
    "#         if self.activation_function == self.sigmoid:\n",
    "#             return value * (1 - value)\n",
    "#         elif self.activation_function == self.relu:\n",
    "#             return 1 if value > 0 else 0\n",
    "#         # Add more functions like tanh if necessary\n",
    "#         return 1  # Default to identity function\n",
    "\n",
    "#     def sigmoid(self, x: float) -> float:\n",
    "#         \"\"\" Sigmoid activation function \"\"\"\n",
    "#         return 1 / (1 + math.exp(-x))\n",
    "\n",
    "#     def relu(self, x: float) -> float:\n",
    "#         \"\"\" ReLU activation function \"\"\"\n",
    "#         return max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model.\n",
      "Training iteration: 0\n",
      "Training iteration: 1000\n",
      "Training iteration: 2000\n",
      "Training iteration: 3000\n",
      "Training iteration: 4000\n",
      "Training iteration: 5000\n",
      "Training iteration: 6000\n",
      "Training iteration: 7000\n",
      "Training iteration: 8000\n",
      "Training iteration: 9000\n",
      "1 input: [0, 0, 0, 1], truth: 1 prediction: [2.365934613334732]\n",
      "2 input: [0, 0, 1, 0], truth: 2 prediction: [2.3219308212158394]\n",
      "3 input: [0, 0, 1, 1], truth: 3 prediction: [4.687865434550571]\n",
      "4 input: [0, 1, 0, 0], truth: 4 prediction: [3.3323830576151225]\n",
      "5 input: [0, 1, 0, 1], truth: 5 prediction: [5.698317670949854]\n",
      "6 input: [0, 1, 1, 0], truth: 6 prediction: [5.654313878830963]\n",
      "7 input: [0, 1, 1, 1], truth: 7 prediction: [8.020248492165695]\n",
      "8 input: [1, 0, 0, 0], truth: 8 prediction: [7.209885092570336]\n",
      "9 input: [1, 0, 0, 1], truth: 9 prediction: [9.575819705905069]\n",
      "10 input: [1, 0, 1, 0], truth: 10 prediction: [9.531815913786176]\n",
      "11 input: [1, 0, 1, 1], truth: 11 prediction: [11.897750527120909]\n",
      "12 input: [1, 1, 0, 0], truth: 12 prediction: [10.542268150185459]\n",
      "13 input: [1, 1, 0, 1], truth: 13 prediction: [12.90820276352019]\n",
      "14 input: [1, 1, 1, 0], truth: 14 prediction: [12.864198971401297]\n",
      "15 input: [1, 1, 1, 1], truth: 15 prediction: [15.230133584736029]\n",
      "R2: 0.9558232537560002\n"
     ]
    }
   ],
   "source": [
    "ITERATION = int(1e4)\n",
    "STEP = 1e-4\n",
    "l1 = Layer(neurons=[Neuron(m=random()) for i in range(0, 4)], step=STEP)\n",
    "l2 = Layer(neurons=[Neuron(m=random()) for i in range(0, 8)], step=STEP)\n",
    "l3 = Layer(neurons=[Neuron(m=random())], step=STEP)\n",
    "n1 = Network([l1, l2, l3])\n",
    "\n",
    "print(\"Training the model.\")\n",
    "for i in range(0, ITERATION):\n",
    "    if i % 1e3 == 0:\n",
    "        print(f\"Training iteration: {i}\")\n",
    "    rn = randint(1, 15)\n",
    "    b = [int(n) for n in bin(rn)[2:]]\n",
    "    while len(b) < 4:\n",
    "        b = [0] + b\n",
    "    n1.train(b, [rn])\n",
    "\n",
    "ys = []\n",
    "ys_predicted = []\n",
    "for i in range(1, 16):\n",
    "    b = [int(n) for n in bin(i)[2:]]\n",
    "    while len(b) < 4:\n",
    "        b = [0] + b\n",
    "    pn = n1.predict(b)\n",
    "    ys.append(i)\n",
    "    ys_predicted.append(pn[0])\n",
    "    print(f\"{i} input: {json.dumps(b)}, truth: {i} prediction: {json.dumps(pn)}\")\n",
    "\n",
    "R2 = r2_score(ys, ys_predicted)\n",
    "\n",
    "print(f\"R2: {R2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
